{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf86062-5ddb-4d7b-b35c-04dacc57869d",
   "metadata": {},
   "source": [
    "# Code status - Joanna\n",
    "\n",
    "## What's covered:\n",
    "- Creates 3 scattered plots showing the for ANS scores vs other scores\n",
    "- Creates the Spearman Rank Correlation Coefficient (rho) for ANS scores vs other scores\n",
    "- Creates a table showing the rho and P value for ANS scores vs other scores\n",
    "\n",
    "## Any bugs üêõ?\n",
    "- No bugs, everything works perfectly üéâ\n",
    "\n",
    "## Potential improvements üìñ:\n",
    "- Calculate the P value by code instead of manually computing using external software (https://www.graphpad.com/quickcalcs/pvalue1.cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c6bcddc-5783-44d0-9bb4-c9bdd7e736a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as m\n",
    "import csv\n",
    "\n",
    "def scatter_plots():\n",
    "    # Create a list of all tests\n",
    "    test_list = ['ans', 'math', 'spatial', 'memory']\n",
    "    \n",
    "    # Create dictionaries to store the row items\n",
    "    # within the accuracy rate and rank column as strings\n",
    "    # including the header\n",
    "    accuracy_col_dict = {}\n",
    "    rank_col_dict = {}\n",
    "    \n",
    "    # Create a dictionary to store the scores for the tests and the rank\n",
    "    # by each individual as integers\n",
    "    accuracy_int_dict = {}\n",
    "    rank_int_dict = {}\n",
    "    \n",
    "    def get_score(test, column_index1, column_index2):\n",
    "        # Opens and reads the test csv file\n",
    "        file = open('Results.csv')\n",
    "        csvreader = csv.reader(file)\n",
    "    \n",
    "        # Add the test score and test score rank to the list value\n",
    "        for row in csvreader:\n",
    "            accuracy_col_dict[test].append(row[column_index1])\n",
    "            rank_col_dict[test].append(row[column_index2])\n",
    "        \n",
    "        # Close the file after processing\n",
    "        file.close()\n",
    "    \n",
    "        return accuracy_col_dict\n",
    "    \n",
    "    i = 1\n",
    "    \n",
    "    \n",
    "    for test in test_list:\n",
    "    \n",
    "        # i is the corresponding column for the scores of each test\n",
    "        # u is the corresponding column for the ranks of the scores of each test\n",
    "        i = i + 1\n",
    "        u = i + 4\n",
    "    \n",
    "        # Insert key:value pairs in the dictionaries\n",
    "        # Key is the test, value is an empty list to accept test scores\n",
    "        accuracy_col_dict[test] = []\n",
    "        accuracy_int_dict[test] = []\n",
    "    \n",
    "        # Key is the test, value is an empty list to accept test score ranks\n",
    "        rank_col_dict[test] = []\n",
    "        rank_int_dict[test] = []\n",
    "    \n",
    "        # Record the scores and score ranks for each test\n",
    "        get_score(test, i, u)\n",
    "    \n",
    "        # Remove the column names from the lists\n",
    "        for value in accuracy_col_dict[test]:\n",
    "            if value != f\"{test}_correct_rate\":\n",
    "                accuracy_int_dict[test].append(float(value))\n",
    "    \n",
    "        for value in rank_col_dict[test]:\n",
    "            if value != f\"{test}_rank\":\n",
    "                rank_int_dict[test].append(float(value))\n",
    "    \n",
    "    # Plot for ANS against Math, Spatial, and Memory abilities\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = m.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    ax1.plot(accuracy_int_dict['ans'], accuracy_int_dict['math'], \"c.\")\n",
    "    ax2.plot(accuracy_int_dict['ans'], accuracy_int_dict['spatial'], \"c.\")\n",
    "    ax3.plot(accuracy_int_dict['ans'], accuracy_int_dict['memory'], \"c.\")\n",
    "    \n",
    "    # Axis labels are set\n",
    "    # x and y axis bounds are set to view the scatter in scale\n",
    "    # Bounds are slightly over 0 and 1\n",
    "    # to improve the visibility of dots at exactly those values to readers\n",
    "    ax1.set_xlabel('ANS score')\n",
    "    ax1.set_ylabel('Math score')\n",
    "    ax1.set_xbound(lower=0, upper=1)\n",
    "    ax1.set_ybound(lower=-0.05, upper=1.05)\n",
    "    \n",
    "    ax2.set_xlabel('ANS score')\n",
    "    ax2.set_ylabel('Spatial score')\n",
    "    ax2.set_xbound(lower=0, upper=1)\n",
    "    ax2.set_ybound(lower=-0.05, upper=1.05)\n",
    "    \n",
    "    ax3.set_xlabel('ANS score')\n",
    "    ax3.set_ylabel('Memory score')\n",
    "    ax3.set_xbound(lower=0, upper=1)\n",
    "    ax3.set_ybound(lower=-0.05, upper=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7692008-0919-4d81-8299-a5af23d86cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'math': -0.13236062717770025, 'spatial': 0.0702526132404181, 'memory': 0.12358449477351918}\n"
     ]
    }
   ],
   "source": [
    "def stats_joanna():\n",
    "    # Create dictionaries to store the square of the rank differences\n",
    "    # and the final rho value\n",
    "    # for ANS with each of the other tests\n",
    "    d_square_dict = {}\n",
    "    rho_dict = {}\n",
    "    \n",
    "    # Define the n as the number of scores collected\n",
    "    # and df as the degree of freedom\n",
    "    n = len(rank_int_dict['ans'])\n",
    "    df = n - 2\n",
    "    \n",
    "    # Define the function to calculate the Spearman's rank coefficient (rho)\n",
    "    def calculate_rho(test):\n",
    "        for i in range(n):\n",
    "            d_square_dict[test].append((rank_int_dict['ans'][i] - rank_int_dict[test][i])**2)\n",
    "    \n",
    "        # Calculate the sum of the d squares for each test with ANS\n",
    "        # and record them in the same dictionary\n",
    "        d_square_dict[f'{test}_sum'] = sum(d_square_dict[test])\n",
    "    \n",
    "        rho_dict[test] = 1 - (6*(d_square_dict[f'{test}_sum']))/(n*(n**2-1))\n",
    "    \n",
    "        return d_square_dict, rho_dict\n",
    "    \n",
    "    # Calculate the rho of the ans score\n",
    "    # and each of the other test scores\n",
    "    for test in test_list:\n",
    "        if test != 'ans':\n",
    "            d_square_dict[test] = []\n",
    "            calculate_rho(test)\n",
    "    \n",
    "    print(rho_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0362649-4002-40e5-812c-89a192e07f88",
   "metadata": {},
   "source": [
    "|   |ANS and Math|ANS and Spatial|ANS and Memory|\n",
    "|---|------------|---------------|--------------|\n",
    "|rho|   -0.132   |     0.070     |     0.124    |\n",
    "|P  |    0.409   |     0.663     |     0.441    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9db8bb9a-de98-4297-832f-6fc2a2e0a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the mean and standard deviation for the scores of each test\n",
    "# # and store in dictionaries\n",
    "# mean_dict = {}\n",
    "# sd_dict = {}\n",
    "\n",
    "# for test in test_list:\n",
    "#     mean_dict[test] = sum(accuracy_int_dict[test])/n\n",
    "#     total_diff = 0\n",
    "#     for i in range(n):\n",
    "#         total_diff += (accuracy_int_dict[test][i] - mean_dict[test])**2\n",
    "\n",
    "#     sd_dict[test] = (total_diff/n)**(1/2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
